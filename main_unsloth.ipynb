{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMC57jeQlBtX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "dl_project_path ='MyDrive/Semester 3/DL/Code'\n",
        "\n",
        "env_path = f'/content/drive/{dl_project_path}'\n",
        "\n",
        "import sys\n",
        "# Add the handout folder to python paths\n",
        "if env_path not in sys.path:\n",
        "    sys.path.append(env_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vlLdlYkeipmt"
      },
      "outputs": [],
      "source": [
        "# Installation of HuggingFace datasets\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install --upgrade peft\n",
        "!pip install safetensors\n",
        "!pip install evaluate\n",
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAxmxAt-LhyF"
      },
      "source": [
        "# Model, tokenizer and device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkIRS81yLgYd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "from transformers import LlamaForCausalLM, AutoTokenizer, get_scheduler, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset\n",
        "from bitsandbytes.optim import AdamW8bit, PagedAdamW8bit\n",
        "from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "from utils import sample_selector, tokenization, dataset_concatenator, AttributeCollate, AlignmentTrainer, save_checkpoint, load_checkpoint, set_datasets\n",
        "\n",
        "# Device\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU available')\n",
        "    device = 'cuda'\n",
        "print(f'Device: {device}')\n",
        "\n",
        "# Seeds\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "\n",
        "# TODO\n",
        "training_path = \"custom_model_samestart_adaptive_v3/\"\n",
        "resume = False\n",
        "\n",
        "save_path = os.path.join(env_path, training_path)\n",
        "load_path = save_path\n",
        "\n",
        "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
        "\n",
        "if resume:\n",
        "  # get last epoch number from the dir\n",
        "  if os.path.exists(load_path):\n",
        "    start_epoch = len(os.listdir(load_path))\n",
        "    epoch_folder = os.path.join(load_path, f\"epoch_{start_epoch-1}\")\n",
        "    model_name = f\"{epoch_folder}/lora_model\"\n",
        "    print(f\"Resuming from epoch {start_epoch}\")\n",
        "  else:\n",
        "    print(\"No checkpoints found\")\n",
        "    start_epoch = 0\n",
        "else:\n",
        "  start_epoch = 0\n",
        "  print(\"Starting from scratch\")\n",
        "\n",
        "# Load the tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=2048,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "if resume:\n",
        "  print(model.print_trainable_parameters())\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "bos_token = tokenizer.bos_token\n",
        "eos_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZzSc0TBMOiK"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncaXWt3p5Ixz"
      },
      "source": [
        "## Instruction dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TQSRstE5Ix0"
      },
      "outputs": [],
      "source": [
        "# Instruction dictionary\n",
        "system_instruction = bos_token + 'System Prompt: Answer the following user instruction based on the provided alignment attributes. Alignment attributes: '\n",
        "user_instruction = 'User instruction: '\n",
        "response_instruction = 'Response: '\n",
        "\n",
        "instruct_dictionary = {'system': system_instruction, 'user': user_instruction, 'response': response_instruction}\n",
        "\n",
        "# Tokenize instruction dictionary\n",
        "instruct_dictionary_tokenized = {}\n",
        "instructions = [instruction for instruction in instruct_dictionary.keys()]\n",
        "\n",
        "for instruction in instructions:\n",
        "\n",
        "    tokens = tokenizer(instruct_dictionary[instruction], padding = False, add_special_tokens=False)\n",
        "    instruct_dictionary_tokenized[f\"{instruction}_labels\"] = torch.Tensor([-100] * len(tokens[\"input_ids\"])).long() # Labels\n",
        "    for key, value in tokens.items():\n",
        "        instruct_dictionary_tokenized[f\"{instruction}_{key}\"] = torch.Tensor(value).long() # IDs and attention mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx2EFx-p5Ix2"
      },
      "source": [
        "## HelpSteer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRnrKuCl5Ix3"
      },
      "outputs": [],
      "source": [
        "dir = os.path.join(env_path, 'dataset')\n",
        "max_length = 1500\n",
        "attributes = ['helpfulness', 'coherence', 'verbosity', 'correctness', 'complexity']\n",
        "train_dataset, val_dataset, test_dataset = set_datasets(dir, tokenizer, instruct_dictionary_tokenized, max_length, attributes)\n",
        "\n",
        "print('Training dataset length: ', len(train_dataset))\n",
        "print('Length of val dataset:', len(val_dataset))\n",
        "print('Length of test dataset:', len(test_dataset))\n",
        "\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXAyNW8beckS"
      },
      "outputs": [],
      "source": [
        "  # train_dataset = train_dataset.select(range(100))\n",
        "# val_dataset = val_dataset.select(range(100))\n",
        "# test_dataset = test_dataset.select(range(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXHYk-lo8Vpx"
      },
      "outputs": [],
      "source": [
        "# Define training loop\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "bos_token_id = tokenizer.bos_token_id\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "if start_epoch == 0:  # Start fresh\n",
        "    # Lora config\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = 16,\n",
        "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],   # recommended to use all modules\n",
        "        lora_alpha = 16,\n",
        "        lora_dropout = 0,\n",
        "        bias = 'none',\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "        random_state = 3407,\n",
        "        use_rslora = False,\n",
        "        loftq_config = None,\n",
        "    )\n",
        "\n",
        "    print(model.print_trainable_parameters())\n",
        "\n",
        "    ## Init DataLoaders\n",
        "    # TODO: set number of attributes and initial probabilities\n",
        "    initial_attributes = ['helpfulness', 'coherence', 'verbosity']\n",
        "    initial_probs = [0.333, 0.333, 0.333]\n",
        "    num_attributes_per_batch = 1\n",
        "\n",
        "    AttrAligner = AlignmentTrainer(alpha=0.5, beta=0.3, T=0.03, attributes=initial_attributes, attribute_probs=initial_probs, num_attributes_per_batch=num_attributes_per_batch)\n",
        "\n",
        "    training_dict = {\"epoch\": start_epoch,\n",
        "                     \"attributes\": initial_attributes,\n",
        "                     \"probs\": initial_probs,\n",
        "                     \"num_attributes_per_batch\": num_attributes_per_batch} # Dictionary for checkpoints\n",
        "\n",
        "    collate_fn = AttributeCollate(attributes = initial_attributes,\n",
        "                                  attribute_probs = initial_probs,\n",
        "                                  num_attributes_per_batch = num_attributes_per_batch,\n",
        "                                  dict_instruct = instruct_dictionary_tokenized,\n",
        "                                  bos_id = bos_token_id,\n",
        "                                  eos_id = eos_token_id)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "    # BATCH TEST\n",
        "    batch = next(iter(train_loader))\n",
        "\n",
        "    print(type(batch))\n",
        "    example = batch['input_ids'][1]\n",
        "    print(tokenizer.decode(example))\n",
        "    print(batch['input_ids'][1].shape)\n",
        "    print(batch['attention_mask'][1].shape)\n",
        "    print(batch['labels'][1].shape)\n",
        "\n",
        "    # Init optimizer and scheduler\n",
        "    if num_attributes_per_batch == len(initial_attributes): # Use all attributes during training, unique optimizer and learning rate\n",
        "\n",
        "      # Instantiate optimizer for a new training session\n",
        "      optimizers = {\"all\": AdamW(model.parameters(), lr=5e-6, weight_decay=1e-2)} # placeholder for the number of optimizers\n",
        "\n",
        "    else:\n",
        "      # learning_rates = {initial_attributes[i]: 1e-5 for i in range(len(initial_attributes))}\n",
        "      optimizers = {initial_attributes[i]: AdamW(model.parameters(), lr=5e-6, weight_decay=1e-2) for i in range(len(initial_attributes))}\n",
        "\n",
        "    # Schedulers\n",
        "    # lr_schedulers = {key: CyclicLR(optimizers[key], base_lr=1e-6, max_lr=5e-6, step_size_up=2000) for key in optimizers}\n",
        "\n",
        "\n",
        "else:\n",
        "\n",
        "    # Load from checkpoint\n",
        "    previous_epoch = start_epoch - 1  # Assuming checkpoint directory structure\n",
        "    training_dict, optimizers = load_checkpoint(load_path, previous_epoch, model)\n",
        "\n",
        "    # Instantiate Dataloader\n",
        "    # Correct this, save train_dataset, val_dataset, instruct_dictionary_tokenized in some path\n",
        "    initial_attributes = training_dict['attributes']\n",
        "    probs = training_dict['probs']\n",
        "    num_attributes_per_batch = training_dict['num_attributes_per_batch']\n",
        "    bos_token_id = tokenizer.bos_token_id\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    AttrAligner = AlignmentTrainer(alpha=0.5, beta=0.9, T=1, attributes=initial_attributes, attribute_probs=probs, num_attributes_per_batch=num_attributes_per_batch)\n",
        "\n",
        "    collate_fn = AttributeCollate(attributes = initial_attributes,\n",
        "                                  attribute_probs = probs,\n",
        "                                  num_attributes_per_batch = num_attributes_per_batch,\n",
        "                                  dict_instruct = instruct_dictionary_tokenized,\n",
        "                                  bos_id = bos_token_id,\n",
        "                                  eos_id = eos_token_id)\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "    # BATCH TEST\n",
        "    batch = next(iter(train_loader))\n",
        "\n",
        "    print(type(batch))\n",
        "    example = batch['input_ids'][1]\n",
        "    print(tokenizer.decode(example))\n",
        "    print(batch['input_ids'][1].shape)\n",
        "    print(batch['attention_mask'][1].shape)\n",
        "    print(batch['labels'][1].shape)\n",
        "\n",
        "print(f\"Training will resume from epoch {start_epoch}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mY5K8974qc-i"
      },
      "outputs": [],
      "source": [
        "# ------TRAIN LOOP------\n",
        "from tqdm.auto import tqdm\n",
        "import evaluate\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "left_training_steps = (num_epochs - start_epoch) * len(train_loader)\n",
        "progress_bar = tqdm(range(left_training_steps))\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "    tot_train_loss = 0\n",
        "    tot_val_loss = 0\n",
        "\n",
        "    # specific train loss and courter per attribute\n",
        "    train_losses = {key: 0 for key in optimizers}\n",
        "    train_counters = {key: 0 for key in optimizers}\n",
        "\n",
        "    # specific val loss and courter per attribute\n",
        "    val_losses = {key: 0 for key in optimizers}\n",
        "    val_counters = {key: 0 for key in optimizers}\n",
        "\n",
        "    # ------ EPOCH LOOP------\n",
        "    probs_str = [np.round(p, 3) for p in collate_fn.attribute_probs]\n",
        "    print(f\"epoch: {epoch}, attributes probabilities: {probs_str}\")\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch_attribute = batch.pop('attributes')\n",
        "        # batch to device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Forward, loss, backprop, optimize\n",
        "        with autocast(device_type='cuda', dtype=dtype):\n",
        "          outputs = model(**batch)\n",
        "          loss = outputs.loss\n",
        "\n",
        "        tot_train_loss += loss.item()\n",
        "\n",
        "        # Optimizer, scheduler, loss\n",
        "        if len(batch_attribute) == len(initial_attributes): # All attributes\n",
        "          optimizer = optimizers[\"all\"]\n",
        "          # lr_scheduler = lr_schedulers[\"all\"]\n",
        "\n",
        "        else: # Attribute - dependent\n",
        "          optimizer = optimizers[batch_attribute[0]]\n",
        "          # lr_scheduler = lr_schedulers[batch_attribute[0]]\n",
        "          train_losses[batch_attribute[0]] += loss.item()\n",
        "          train_counters[batch_attribute[0]] += 1\n",
        "\n",
        "        # Step\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        # lr_scheduler.step()\n",
        "\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # ------ VALIDATION LOOP------\n",
        "    model.eval()\n",
        "    for batch in val_loader:\n",
        "      batch_attribute = batch.pop('attributes')\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      with torch.no_grad(), autocast(device_type='cuda', dtype=dtype):\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "      tot_val_loss += loss.item()\n",
        "\n",
        "      # Loss\n",
        "      if len(batch_attribute) != len(initial_attributes): # Independent attributes\n",
        "        val_losses[batch_attribute[0]] += loss.item()\n",
        "        val_counters[batch_attribute[0]] += 1\n",
        "\n",
        "\n",
        "    if len(batch_attribute) != len(initial_attributes):\n",
        "      # Average loss\n",
        "      for key in train_losses:\n",
        "        if train_counters[key] != 0:\n",
        "          train_losses[key] /= train_counters[key]\n",
        "\n",
        "      for key in val_losses:\n",
        "        if val_counters[key] != 0:\n",
        "          val_losses[key] /= val_counters[key]\n",
        "\n",
        "      # set new probabilities\n",
        "      new_probs = AttrAligner.compute_new_probs(train_losses, val_losses)\n",
        "      collate_fn.set_attribute_probs(new_probs)\n",
        "\n",
        "      training_dict[\"probs\"] = new_probs # Update probabilities\n",
        "\n",
        "    # Add all loss\n",
        "    train_losses[\"all\"] = tot_train_loss/len(train_loader)\n",
        "    val_losses[\"all\"] = tot_val_loss/len(val_loader)\n",
        "\n",
        "    # Epoch end\n",
        "    print(f\"Epoch {epoch}/{num_epochs - 1} - \" + \", \".join([f\"{attr}: Train Loss: {train_losses[attr]:.4f}, Val Loss: {val_losses[attr]:.4f}\" for attr in train_losses]))\n",
        "    save_checkpoint(save_path, epoch, model, training_dict, optimizers, train_losses, val_losses)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnibhpzR6O84"
      },
      "source": [
        "# Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxxrfCCCK4yx"
      },
      "outputs": [],
      "source": [
        "# Normal tokenization and loading\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "tokenizer2.pad_token = tokenizer2.eos_token\n",
        "\n",
        "def tokenizer2(data):\n",
        "  return tokenizer(data['prompt'], max_length = 1000, padding = 'max_length', truncation=True)\n",
        "\n",
        "small_set = train_dataset\n",
        "train_set2 = small_set.map(tokenizer2)\n",
        "# select only columns: prompt_input_ids, prompt_attention_mask, response_input_ids, response_attention_mask\n",
        "train_set2 = train_set2.select_columns(['input_ids', 'attention_mask'])\n",
        "train_set2.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "train_loader2 = torch.utils.data.DataLoader(train_set2, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eogdpCIyM_YG"
      },
      "outputs": [],
      "source": [
        "# BATCH TEST\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "print(type(batch))\n",
        "example = batch['input_ids'][1]\n",
        "print(tokenizer.decode(example))\n",
        "print(batch['input_ids'][1].shape)\n",
        "print(batch['attention_mask'][1].shape)\n",
        "print(batch['labels'][1].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-pKNiXmJQUf"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      outputs = model.generate(**batch, max_new_tokens=1000)\n",
        "\n",
        "#input = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)\n",
        "out = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42zRkY_UJ4as"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}