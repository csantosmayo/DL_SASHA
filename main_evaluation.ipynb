{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgjRkpfV44olhXh+5OrBe7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3-TU7ZWx8YGn"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","dl_project_path ='MyDrive/ETH/DL_PROJECT/MAIN'\n","\n","env_path = f'/content/drive/{dl_project_path}'\n","\n","import sys\n","# Add the handout folder to python paths\n","if env_path not in sys.path:\n","    sys.path.append(env_path)"]},{"cell_type":"code","source":["# Installation of HuggingFace datasets\n","!pip install openai"],"metadata":{"collapsed":true,"id":"_5XujKbP8cLq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","\n","from openai import OpenAI"],"metadata":{"id":"Ti2BGxkm8izv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#loading baseline responses (and prompts)\n","import pickle\n","\n","# the folder of the baseline pre-trained model could be different from the folder of the custom SASHA model if number of attributes was set to 3\n","# if the text generation mode was 1 attribute at a time then the baseline and SASHA models will be in the same folder\n","baseline_path = 'inference_all_1_attr' # specify the folder in which the baseline model is saved\n","baseline_path = os.path.join(env_path, baseline_path)\n","\n","with open(os.path.join(baseline_path, 'prompts.pkl'), 'rb') as f:\n","  baseline_prompts = pickle.load(f)\n","\n","with open(os.path.join(baseline_path, 'responses.pkl'), 'rb') as f:\n","  baseline_responses = pickle.load(f)\n","\n","baseline_responses = baseline_responses['base_model']"],"metadata":{"id":"qS1EBJar0TZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the inference responses from the local dir\n","inference_folder = 'inference_all_1_attr' # specify the folder in which the custom model is saved\n","inference_path = os.path.join(env_path, inference_folder)\n","\n","with open(os.path.join(inference_path, 'prompts.pkl'), 'rb') as f:\n","  custom_prompts = pickle.load(f)\n","\n","with open(os.path.join(inference_path, 'responses.pkl'), 'rb') as f:\n","  custom_responses = pickle.load(f)\n","\n","print(f'Responses keys: {custom_responses.keys()}') # print all available custom models in the folder"],"metadata":{"id":"YADJt8rA-RgH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print samples of baseline and custom prompts to ensure correct loading\n","#print(baseline_prompts[0])\n","#print(baseline_prompts[2])\n","\n","#print(custom_prompts[0])\n","#print(custom_prompts[2])"],"metadata":{"collapsed":true,"id":"PrBMeIT25-P7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# choose which model to evaluate and combine baseline and model responses inside the responses dict --> comparison between baseline model and one SASHA model at a time\n","model = 'custom_model_samestart_adaptive_v3' # TODO\n","responses = {'Baseline': baseline_responses, 'Custom': custom_responses[model]}\n","prompts = baseline_prompts"],"metadata":{"id":"briR0E2D04-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pydantic import BaseModel, validator, ValidationError\n","\n","# Definition of response format object\n","class Response(BaseModel):\n","  score_A: int\n","  score_B: int\n","\n","  @validator(\"score_A\", \"score_B\")\n","  def scores_between_zero_and_ten(cls, v):\n","      if not (0 <= v <= 10):\n","          raise ValidationError(\"Score must be between 0 and 10 inclusive.\")\n","      return v\n"],"metadata":{"id":"z6JBuxkCQJm8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = []\n","system_message = \"\"\"You are a helpful assistant that evaluates how well each response aligns with the given prompt, provided the specific attributes (each ranging between 0 and 4) included before the prompt itself.\n","You must output valid JSON with the following structure:\n","\n","{\n","  \"score_A\": 0,\n","  \"score_B\": 0\n","}\n","\n","No additional keys or text.\n","All scores must be integers between 0 and 10, with perfect alignment having score 10.\n","\"\"\"\n","\n","client = OpenAI(api_key = open(f'{env_path}/OpenAI_API_key.txt', 'r').read())\n","\n","responses_order = [] # 0 if baseline response comes first\n","\n","for i in range(len(prompts)):\n","  prompt = prompts[i]\n","  # randomly select the order of the baseline model answer and the custom model answer\n","  if np.random.rand() < 0.5:\n","    response_A = responses['Baseline'][i]\n","    response_B = responses['Custom'][i]\n","    responses_order.append(0)\n","  else:\n","    response_A = responses['Custom'][i]\n","    response_B = responses['Baseline'][i]\n","    responses_order.append(1)\n","  messages = [{'role': 'system', 'content': system_message},\n","             {'role': 'user', 'content': f'PROMPT:\\n{prompt}\\nRESPONSE A:\\n{response_A}\\nRESPONSE B:\\n{response_B}'}]\n","\n","  try:\n","    completion = client.beta.chat.completions.parse(\n","      model=\"gpt-4o\",\n","      messages=messages,\n","      response_format=Response)\n","\n","    evaluation = completion.choices[0].message.parsed\n","    results.append(evaluation)\n","\n","  except ValidationError:\n","    continue\n"],"metadata":{"id":"2XKLTfBmHzqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print len results\n","print(len(results))"],"metadata":{"id":"bEn5PAJ8RZSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extraction of the evaluation results\n","scores = {'Baseline': [], 'Custom': []}\n","\n","for i, result in enumerate(results):\n","  if responses_order[i] == 0:\n","    scores['Baseline'].append(result.score_A)\n","    scores['Custom'].append(result.score_B)\n","  else:\n","    scores['Baseline'].append(result.score_B)\n","    scores['Custom'].append(result.score_A)"],"metadata":{"id":"bLi7d15Po5kX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# computation of the average score and standard deviation for each one of the models\n","mean_scores = {'Baseline': 0, 'Custom': 0}\n","std_scores = {'Baseline':0, 'Custom': 0}\n","custom_win_rate = 0\n","\n","for model in scores.keys():\n","  mean_scores[model] = np.mean(scores[model])\n","  std_scores[model] = np.std(scores[model])\n","\n","custom_win_rate = 100 * np.sum(np.array(scores['Custom']) > np.array(scores['Baseline'])) / len(scores['Custom'])\n","ties_percentage = 100 * np.sum(np.array(scores['Custom']) == np.array(scores['Baseline'])) / len(scores['Custom'])\n","print(f'Mean scores: {mean_scores}')\n","print(f'Standard deviation of scores: {std_scores}')\n","print(f'Custom model win rate: {custom_win_rate}%')\n","print(f'Ties percentage: {ties_percentage}%')\n"],"metadata":{"id":"vlDfZdhoXwbw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save scores, custom win rate and ties in the evaluation folder\n","evaluation_folder = 'evaluation_4_4o_1attr'\n","evaluation_path = os.path.join(env_path, evaluation_folder)\n","\n","if not os.path.exists(evaluation_path):\n","    os.makedirs(evaluation_path)\n","\n","with open(os.path.join(evaluation_path, 'scores.pkl'), 'wb') as f:\n","  pickle.dump(scores, f)\n","\n","with open(os.path.join(evaluation_path, 'mean_scores.pkl'), 'wb') as f:\n","  pickle.dump(mean_scores, f)\n","\n","with open(os.path.join(evaluation_path, 'std_scores.pkl'), 'wb') as f:\n","  pickle.dump(std_scores, f)\n","\n","with open(os.path.join(evaluation_path, 'custom_win_rate.pkl'), 'wb') as f:\n","  pickle.dump(custom_win_rate, f)\n","\n","with open(os.path.join(evaluation_path, 'ties_percentage.pkl'), 'wb') as f:\n","  pickle.dump(ties_percentage, f)"],"metadata":{"id":"zV0QSk8FA_Q1"},"execution_count":null,"outputs":[]}]}